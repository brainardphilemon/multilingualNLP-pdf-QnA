{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import pdfplumber\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "!pip install googletrans==3.1.0a0\n",
        "import googletrans"
      ],
      "metadata": {
        "id": "N8_RV2dCGs1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12e0002-0b37-4a90-eca2-dc44981722b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.11/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.11/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.1.31)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import requests\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the translator.\n",
        "translator = Translator()\n",
        "\n",
        "# Define a safety settings object as required by your Gemini API.\n",
        "safety_settings = None  # Update as necessary.\n",
        "\n",
        "\n",
        "def extract_clean_text_chunks_from_pdf(pdf_path, chunk_size=1000):\n",
        "    \"\"\"Extracts and cleans text chunks (of approximately chunk_size characters) from the PDF.\n",
        "    Before chunking, the extracted text is translated into English for processing.\"\"\"\n",
        "    text_chunks = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                page_text = re.sub(r'http\\S+|www\\S+|file:\\S+|\\S+\\.html', '', page_text)\n",
        "                page_text = re.sub(r'\\s+', ' ', page_text).strip()\n",
        "                page_text = translator.translate(page_text, dest='en').text\n",
        "                for i in range(0, len(page_text), chunk_size):\n",
        "                    text_chunks.append(page_text[i:i+chunk_size])\n",
        "    return text_chunks\n",
        "\n",
        "\n",
        "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qa-qg-hl\")\n",
        "\n",
        "def retrieve_relevant_chunks(prompt, text_chunks, top_n=5):\n",
        "    \"\"\"Finds the most relevant text chunks for a given prompt using TF-IDF cosine similarity.\"\"\"\n",
        "    vectorizer = TfidfVectorizer().fit_transform([prompt] + text_chunks)\n",
        "    cosine_similarities = cosine_similarity(vectorizer[0:1], vectorizer[1:]).flatten()\n",
        "    top_n_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "    return [text_chunks[i] for i in top_n_indices]\n",
        "\n",
        "def generate_questions_t5(text, num_questions=10):\n",
        "    \"\"\"Generates unique brief questions from text using a T5-based question generation model.\"\"\"\n",
        "    prompt_text = \"generate brief question: \" + text\n",
        "    questions = question_generator(prompt_text, max_length=100, num_beams=5, num_return_sequences=num_questions)\n",
        "    unique_questions = set(q['generated_text'].strip().replace(\"\\n\", \" \") for q in questions)\n",
        "    return list(unique_questions)\n",
        "\n",
        "def generate_questions_gemini(text, num_questions=10):\n",
        "    \"\"\"\n",
        "    Generates unique brief questions from aggregated text using the Gemini model.\n",
        "    Instructs Gemini to output ONLY the questions, labeled with numbers 1 through num_questions.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        f\"Generate exactly {num_questions} brief questions about the following text. \"\n",
        "        f\"Return only the questions, labeled with numbers 1 through {num_questions}, and nothing else. \"\n",
        "        f\"Do not provide any introductions or summaries.\\n\\n\"\n",
        "        f\"Text:\\n{text}\"\n",
        "    )\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=[prompt],\n",
        "        config=types.GenerateContentConfig(safety_settings=safety_settings)\n",
        "    )\n",
        "    questions_list = []\n",
        "    if response and response.text.strip():\n",
        "        lines = response.text.strip().split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and (line[0].isdigit() or line.startswith(f\"{len(questions_list)+1}.\")):\n",
        "                cleaned_line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line).strip()\n",
        "                questions_list.append(cleaned_line)\n",
        "    return questions_list\n",
        "\n",
        "def generate_answers_for_questions_gemini(questions, context):\n",
        "    \"\"\"\n",
        "    Aggregates multiple questions into a single prompt and calls Gemini once to get answers.\n",
        "    The prompt instructs Gemini to label each answer with its corresponding question number.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Answer the following questions concisely (4-5 lines max) based on the context provided. \"\n",
        "        \"Provide each answer labeled with its corresponding question number.\\n\\n\"\n",
        "        f\"Context: {context}\\n\\nQuestions:\\n\"\n",
        "    )\n",
        "    for i, q in enumerate(questions, start=1):\n",
        "        prompt += f\"{i}. {q}\\n\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=[prompt],\n",
        "        config=types.GenerateContentConfig(safety_settings=safety_settings)\n",
        "    )\n",
        "\n",
        "    answers = {}\n",
        "    if response and response.text.strip():\n",
        "        lines = response.text.strip().split('\\n')\n",
        "        current_index = None\n",
        "        current_answer = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if line[0].isdigit() and line[1] == '.':\n",
        "                if current_index is not None and current_answer:\n",
        "                    answers[questions[current_index - 1]] = \"\\n\".join(current_answer).strip()\n",
        "                try:\n",
        "                    parts = line.split('.', 1)\n",
        "                    current_index = int(parts[0])\n",
        "                    current_answer = [parts[1].strip()] if len(parts) > 1 else []\n",
        "                except ValueError:\n",
        "                    continue\n",
        "            else:\n",
        "                if current_index is not None:\n",
        "                    current_answer.append(line)\n",
        "        if current_index is not None and current_answer:\n",
        "            answers[questions[current_index - 1]] = \"\\n\".join(current_answer).strip()\n",
        "    return answers\n",
        "\n",
        "\n",
        "def generate_summary_gemini(text, word_count=50):\n",
        "    \"\"\"\n",
        "    Uses the Gemini model to generate a summary of the given text in exactly word_count words.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        f\"Summarize the following text in exactly {word_count} words. \"\n",
        "        \"Do not include any additional commentary or explanations.\\n\\n\"\n",
        "        f\"Text:\\n{text}\"\n",
        "    )\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=[prompt],\n",
        "        config=types.GenerateContentConfig(safety_settings=safety_settings)\n",
        "    )\n",
        "    summary = response.text.strip() if response and response.text.strip() else \"\"\n",
        "    return summary\n",
        "\n",
        "def create_summary_file(text, summary_file_path, word_count=50, output_lang=\"en\"):\n",
        "    \"\"\"Generates a summary using Gemini, translates it to the chosen language, and writes it to a file.\"\"\"\n",
        "    summary = generate_summary_gemini(text, word_count=word_count)\n",
        "    # Translate summary into chosen language before saving.\n",
        "    if output_lang != \"en\":\n",
        "        summary = translator.translate(summary, dest=output_lang).text\n",
        "    with open(summary_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summary)\n",
        "    return summary\n",
        "\n",
        "#########################\n",
        "# Follow-Up Handling Using Summary File\n",
        "#########################\n",
        "def handle_followups(valid_questions, summary_text, aggregated_context, use_t5, output_lang=\"en\"):\n",
        "    \"\"\"\n",
        "    Allows the user to select a generated question and then ask follow-up questions.\n",
        "    In addition to follow-up queries:\n",
        "      - The user can type 'new' (in the inner loop) to generate a new random question.\n",
        "      - The user can enter '0' at the question selection prompt to generate a new random question.\n",
        "    All user inputs (except commands like 'done' or 'new') are translated to English for processing,\n",
        "    and all outputs are translated back to the chosen language.\n",
        "    \"\"\"\n",
        "    questions_list = list(valid_questions.keys())\n",
        "    while True:\n",
        "        display_list = [translator.translate(q, dest=output_lang).text for q in questions_list]\n",
        "        print(\"\\n\" + translator.translate(\"Which question do you want to follow up on? (Enter a number, '0' for a new random question, or 'exit')\", dest=output_lang).text)\n",
        "        for i, q in enumerate(display_list, start=1):\n",
        "            print(f\"{i}. {q[:60]}...\")\n",
        "        choice = input(\"> \").strip().lower()\n",
        "        if choice == \"exit\":\n",
        "            print(translator.translate(\"Exiting follow-up mode.\", dest=output_lang).text)\n",
        "            break\n",
        "\n",
        "        if choice == \"0\":\n",
        "            if use_t5:\n",
        "                new_question = generate_questions_t5(aggregated_context, num_questions=1)[0]\n",
        "            else:\n",
        "                new_question = generate_questions_gemini(aggregated_context, num_questions=1)[0]\n",
        "            new_answer_dict = generate_answers_for_questions_gemini([new_question], aggregated_context)\n",
        "            new_answer = new_answer_dict.get(new_question, translator.translate(\"I'm sorry, I couldn't generate an answer at this time.\", dest=output_lang).text)\n",
        "            print(\"\\n\" + translator.translate(\"New Random Question:\", dest=output_lang).text)\n",
        "            print(translator.translate(new_question, dest=output_lang).text)\n",
        "            print(translator.translate(\"Answer:\", dest=output_lang).text)\n",
        "            print(translator.translate(new_answer, dest=output_lang).text + \"\\n\")\n",
        "            # Allow follow-ups on the newly generated question.\n",
        "            selected_question = new_question\n",
        "            original_answer = new_answer\n",
        "        else:\n",
        "            try:\n",
        "                choice_index = int(choice) - 1\n",
        "                if choice_index < 0 or choice_index >= len(questions_list):\n",
        "                    print(translator.translate(\"Invalid choice. Please try again.\", dest=output_lang).text)\n",
        "                    continue\n",
        "            except ValueError:\n",
        "                print(translator.translate(\"Invalid choice. Please try again.\", dest=output_lang).text)\n",
        "                continue\n",
        "            selected_question = questions_list[choice_index]\n",
        "            original_answer = valid_questions[selected_question]\n",
        "            print(\"\\n\" + translator.translate(\"You selected:\", dest=output_lang).text)\n",
        "            print(translator.translate(selected_question, dest=output_lang).text)\n",
        "\n",
        "        while True:\n",
        "            follow_up = input(\"\\n\" + translator.translate(\"Enter your follow-up question (or type 'done' to pick another question, or 'new' for a new random question):\", dest=output_lang).text + \" \")\n",
        "            cmd = follow_up.strip().lower()\n",
        "            if cmd == \"done\":\n",
        "                break\n",
        "            if cmd == \"new\":\n",
        "                # New random question generation.\n",
        "                if use_t5:\n",
        "                    new_question = generate_questions_t5(aggregated_context, num_questions=1)[0]\n",
        "                else:\n",
        "                    new_question = generate_questions_gemini(aggregated_context, num_questions=1)[0]\n",
        "                new_answer_dict = generate_answers_for_questions_gemini([new_question], aggregated_context)\n",
        "                new_answer = new_answer_dict.get(new_question, translator.translate(\"I'm sorry, I couldn't generate an answer at this time.\", dest=output_lang).text)\n",
        "                print(\"\\n\" + translator.translate(\"New Random Question:\", dest=output_lang).text)\n",
        "                print(translator.translate(new_question, dest=output_lang).text)\n",
        "                print(translator.translate(\"Answer:\", dest=output_lang).text)\n",
        "                print(translator.translate(new_answer, dest=output_lang).text + \"\\n\")\n",
        "                # Update the selected question for subsequent follow-ups.\n",
        "                selected_question = new_question\n",
        "                original_answer = new_answer\n",
        "                continue\n",
        "\n",
        "            # Translate follow-up question into English before processing.\n",
        "            follow_up_eng = follow_up\n",
        "            if output_lang != \"en\":\n",
        "                follow_up_eng = translator.translate(follow_up, dest=\"en\").text\n",
        "\n",
        "            followup_prompt = (\n",
        "                \"Answer the following follow-up question concisely (4-5 lines max) based on the context provided. \"\n",
        "                f\"Original Question: {selected_question}\\n\"\n",
        "                f\"Original Answer: {original_answer}\\n\"\n",
        "                f\"Follow-up question: {follow_up_eng}\\n\\n\"\n",
        "                f\"Context (50-word PDF summary): {summary_text}\\n\"\n",
        "            )\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                contents=[followup_prompt],\n",
        "                config=types.GenerateContentConfig(safety_settings=safety_settings)\n",
        "            )\n",
        "            new_answer = response.text.strip() if response and response.text.strip() else \"I'm sorry, I couldn't generate an answer at this time.\"\n",
        "            print(\"\\n\" + translator.translate(\"Assistant:\", dest=output_lang).text)\n",
        "            print(translator.translate(new_answer, dest=output_lang).text + \"\\n\")\n",
        "        print(translator.translate(\"Returning to question selection menu...\", dest=output_lang).text)\n",
        "\n",
        "#########################\n",
        "# Main Execution\n",
        "#########################\n",
        "def main(pdf_path, prompt, use_t5, output_lang=\"en\"):\n",
        "    # Extract text chunks from the PDF (translated to English for processing).\n",
        "    text_chunks = extract_clean_text_chunks_from_pdf(pdf_path)\n",
        "\n",
        "    # Retrieve the most relevant chunks for the given prompt.\n",
        "    relevant_chunks = retrieve_relevant_chunks(prompt, text_chunks, top_n=5)\n",
        "    aggregated_text = \" \".join(relevant_chunks)\n",
        "\n",
        "    # Generate 10 brief questions using T5 or Gemini.\n",
        "    if use_t5:\n",
        "        all_questions = generate_questions_t5(aggregated_text, num_questions=10)\n",
        "    else:\n",
        "        all_questions = generate_questions_gemini(aggregated_text, num_questions=10)\n",
        "\n",
        "    # Use the entire PDF content as context for generating answers.\n",
        "    aggregated_context = \" \".join(text_chunks)\n",
        "    answers = generate_answers_for_questions_gemini(all_questions, aggregated_context)\n",
        "    valid_questions = {q: answers[q] for q in all_questions if q in answers and answers[q]}\n",
        "\n",
        "    if valid_questions:\n",
        "        print(\"\\n\" + translator.translate(\"Generated Questions & Answers:\", dest=output_lang).text)\n",
        "        idx = 1\n",
        "        for question, answer in valid_questions.items():\n",
        "            print(f\"{idx}. {translator.translate(question, dest=output_lang).text}\")\n",
        "            print(translator.translate(\"Answer:\", dest=output_lang).text)\n",
        "            print(translator.translate(answer, dest=output_lang).text + \"\\n\")\n",
        "            idx += 1\n",
        "    else:\n",
        "        print(translator.translate(\"No relevant questions found in the provided PDF context.\", dest=output_lang).text)\n",
        "        return\n",
        "\n",
        "    # Generate a 50-word summary of the entire PDF context and save it to a file.\n",
        "    summary_file_path = \"pdf_summary.txt\"\n",
        "    summary_text = create_summary_file(aggregated_context, summary_file_path, word_count=50, output_lang=output_lang)\n",
        "    print(\"\\n\" + translator.translate(\"50-word PDF summary saved to\", dest=output_lang).text, summary_file_path)\n",
        "    print(translator.translate(summary_text, dest=output_lang).text + \"\\n\")\n",
        "\n",
        "    # Enter follow-up mode using the summary as context.\n",
        "    print(\"\\n\" + translator.translate(\"--- FOLLOW-UP MODE ---\", dest=output_lang).text)\n",
        "    handle_followups(valid_questions, summary_text, aggregated_context, use_t5, output_lang)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize Gemini API client (replace with your actual API key)\n",
        "    client = genai.Client(api_key=\"AIzaSyB8GJ0UWeKVAdHu8mRzDGDgxWcwX7eyokI\")\n",
        "\n",
        "    # Define language options.\n",
        "    language_options = {\n",
        "        \"1\": \"en\",    # English\n",
        "        \"2\": \"hi\",    # Hindi\n",
        "        \"3\": \"es\",    # Spanish\n",
        "        \"4\": \"fr\",    # French\n",
        "        \"5\": \"de\",    # German\n",
        "        \"6\": \"zh-cn\", # Chinese\n",
        "        \"7\": \"ar\"     # Arabic\n",
        "    }\n",
        "\n",
        "    print(\"Choose your language / अपनी भाषा चुनें / Elija su idioma / Choisissez votre langue / Wählen Sie Ihre Sprache / 选择你的语言 / اختر لغتك:\")\n",
        "    print(\"1. English\\n2. Hindi\\n3. Spanish\\n4. French\\n5. German\\n6. Chinese\\n7. Arabic\")\n",
        "    lang_choice = input(\"Enter the number corresponding to your language: \").strip()\n",
        "    output_lang = language_options.get(lang_choice, \"en\")\n",
        "\n",
        "    pdf_path = input(translator.translate(\"Enter the path to your PDF file: \", dest=output_lang))\n",
        "    prompt = input(translator.translate(\"Enter your prompt for question generation: \", dest=output_lang))\n",
        "    # Translate prompt into English (if necessary) for processing.\n",
        "    if output_lang != \"en\":\n",
        "        prompt = translator.translate(prompt, dest=\"en\").text\n",
        "    use_t5 = input(translator.translate(\"Use T5 model for question generation? (y/n): \", dest=output_lang)).lower() == 'y'\n",
        "    main(pdf_path, prompt, use_t5, output_lang)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DT93J42I-VRe",
        "outputId": "b13382ee-9d40-4c7c-e21a-6386e52d744f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose your language / अपनी भाषा चुनें / Elija su idioma / Choisissez votre langue / Wählen Sie Ihre Sprache / 选择你的语言 / اختر لغتك:\n",
            "1. English\n",
            "2. Hindi\n",
            "3. Spanish\n",
            "4. French\n",
            "5. German\n",
            "6. Chinese\n",
            "7. Arabic\n",
            "Enter the number corresponding to your language: 2\n",
            "Translated(src=en, dest=hi, text=अपनी पीडीएफ फाइल में पथ दर्ज करें:, pronunciation=apanee peedeeeph phail mein path darj karen:, extra_data=\"{'translat...\")/content/RAMAYANA.pdf\n",
            "Translated(src=en, dest=hi, text=प्रश्न पीढ़ी के लिए अपना संकेत दर्ज करें:, pronunciation=prashn peedhee ke lie apana sanket darj karen:, extra_data=\"{'translat...\")हनुमान\n",
            "Translated(src=en, dest=hi, text=प्रश्न पीढ़ी के लिए T5 मॉडल का उपयोग करें? (y/n):, pronunciation=[[None, 'offline']], extra_data=\"{'translat...\")n\n",
            "\n",
            "उत्पन्न प्रश्न और उत्तर:\n",
            "1. लंका के नष्ट होने के बाद हनुमान पर फूलों की पंखुड़ियों को किसने स्नान किया?\n",
            "उत्तर:\n",
            "सूरस (देवताओं) ने लंका के नष्ट होने के बाद हनुमान पर फूलों की पंखुड़ियों की बौछार की।\n",
            "\n",
            "2. किसने अपनी उड़ान में बाधा डालकर राम के प्रति हनुमान की भक्ति का परीक्षण किया?\n",
            "उत्तर:\n",
            "देवों (देवताओं) ने सुरासा को भेजा, जो अपनी उड़ान में बाधा डालकर राम के प्रति हनुमान की भक्ति का परीक्षण करने के लिए एक राक्षस में तब्दील हो गए।\n",
            "\n",
            "3. लंका में हनुमान किस पर्वत पर उतरा?\n",
            "उत्तर:\n",
            "हनुमान लंका के माउंट ट्रिकुटा पर उतरा।\n",
            "\n",
            "4. लंका का रक्षक कौन था जिसने हनुमान को रोकने की कोशिश की थी?\n",
            "उत्तर:\n",
            "लंकिनी लंका की रक्षक थी जिसने हनुमान को शहर में प्रवेश करने से रोकने की कोशिश की।\n",
            "\n",
            "5. हनुमान ने अंततः सीता को कहां पाया?\n",
            "उत्तर:\n",
            "हनुमान ने अंततः अशोक गार्डन में सीता को पाया।\n",
            "\n",
            "6. सीता ने राम को स्मृति चिन्ह के रूप में क्या भेजा?\n",
            "उत्तर:\n",
            "सीता ने अपने एक गहने को राम को एक स्मृति चिन्ह के रूप में भेजा।\n",
            "\n",
            "7. क्यों राम चिंतित थे और सुग्रीवा से अधिक मदद मांगी गई?\n",
            "उत्तर:\n",
            "राम ने सुग्रिवा से अधिक मदद मांगी क्योंकि उन्हें डर था कि रावण बदला लेने के लिए सीता को मार सकता है।\n",
            "\n",
            "8. जब तक वह सीता की खबर नहीं लेता, तब तक हनुमान ने क्या नहीं किया?\n",
            "उत्तर:\n",
            "हनुमान ने शपथ ली कि जब तक वह सीता की खबर नहीं लेता, तब तक वह किशकिंडा नहीं लौटेगा।\n",
            "\n",
            "9. रावण ने हनुमान को कैसे पकड़ लिया?\n",
            "उत्तर:\n",
            "रावण ने अपने बेटे इंद्रजीत को एक कुशल योद्धा भेजा, जिसने हनुमान को कैद करने और उसे रावण के दरबार में खींचने के लिए एक शक्तिशाली हथियार का इस्तेमाल किया।\n",
            "10। हनुमान ने रावण को चेतावनी दी कि उसने सीता का अपहरण करके एक बड़ी गलती की है और अगर वह जीना चाहता है, तो उसे सीता को राम में लौटना चाहिए और उसमें शरण लेना चाहिए; अन्यथा, वह मरने के लिए निश्चित था।\n",
            "\n",
            "\n",
            "50-शब्द पीडीएफ सारांश को बचाया pdf_summary.txt\n",
            "इस बच्चों के रामायण ने राम के जन्म, युवाओं, सीता से शादी और निर्वासन के बारे में बताया।  रानी कैकेई की इच्छाओं ने राम को जंगल में भेज दिया, जहां सीता को रावण द्वारा अपहरण कर लिया गया।  राम सुग्रिवा और हनुमान के साथ सहयोगी, लंका पर युद्ध छेड़ते हुए, रावण को हराकर और सीता को बचाते हुए।  अयोध्या लौटते हुए, राम को राजा का ताज पहनाया गया।\n",
            "\n",
            "\n",
            "--- अनुवर्ती मोड ---\n",
            "\n",
            "आप किस प्रश्न का पालन करना चाहते हैं? (एक नए यादृच्छिक प्रश्न के लिए एक नंबर, '0' दर्ज करें, या 'बाहर निकलें')\n",
            "1. लंका के नष्ट होने के बाद हनुमान पर फूलों की पंखुड़ियों को कि...\n",
            "2. किसने अपनी उड़ान में बाधा डालकर राम के प्रति हनुमान की भक्ति...\n",
            "3. लंका में हनुमान किस पर्वत पर उतरा?...\n",
            "4. लंका का रक्षक कौन था जिसने हनुमान को रोकने की कोशिश की थी?...\n",
            "5. हनुमान ने अंततः सीता को कहां पाया?...\n",
            "6. सीता ने राम को स्मृति चिन्ह के रूप में क्या भेजा?...\n",
            "7. क्यों राम चिंतित थे और सुग्रीवा से अधिक मदद मांगी गई?...\n",
            "8. जब तक वह सीता की खबर नहीं लेता, तब तक हनुमान ने क्या नहीं कि...\n",
            "9. रावण ने हनुमान को कैसे पकड़ लिया?...\n",
            "> 1\n",
            "\n",
            "आपने चुना:\n",
            "लंका के नष्ट होने के बाद हनुमान पर फूलों की पंखुड़ियों को किसने स्नान किया?\n",
            "\n",
            "अपना फॉलो-अप प्रश्न दर्ज करें (या किसी अन्य प्रश्न को चुनने के लिए 'किया गया' टाइप करें, या एक नए यादृच्छिक प्रश्न के लिए 'नया'): हनुमान\n",
            "\n",
            "सहायक:\n",
            "चूंकि संदर्भ रामायण कथा को संक्षेप में प्रस्तुत करता है, इसलिए अनुवर्ती प्रश्न की संभावना अधिक जानकारी * के बारे में * हनुमान के बारे में है। एक संक्षिप्त उत्तर होगा:\n",
            "\n",
            "हनुमान रामायण में एक केंद्रीय व्यक्ति है। वह राम का एक समर्पित भक्त और एक शक्तिशाली बंदर भगवान है। उन्होंने सीता को बचाने में राम को सहायता प्रदान की, जिसमें अपार शक्ति और वफादारी दिखाई दी। रावण पर जीत में उनकी भूमिका महत्वपूर्ण थी।\n",
            "\n",
            "\n",
            "अपना फॉलो-अप प्रश्न दर्ज करें (या किसी अन्य प्रश्न को चुनने के लिए 'किया गया' टाइप करें, या एक नए यादृच्छिक प्रश्न के लिए 'नया'): हनुमान\n",
            "\n",
            "सहायक:\n",
            "चूंकि मूल प्रश्न हनुमान को संदर्भित करता है और उत्तर सूरस की पहचान उस पर फूलों की बौछार के रूप में करता है, अनुवर्ती प्रश्न संभवतः हनुमान के बारे में अधिक जानकारी उस विशिष्ट संदर्भ में अधिक जानकारी लेता है। एक संक्षिप्त उत्तर हो सकता है:\n",
            "\n",
            "हनुमान ने लंका के विनाश में एक महत्वपूर्ण भूमिका निभाई, जिसमें सीता को बचाने में राम का समर्थन किया गया। उनकी बहादुरी और भक्ति ने उन्हें सम्मान और प्रशंसा के संकेत के रूप में सुरस से पंखुड़ियों की बौछार की।\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0b691c02b9cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0muse_t5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use T5 model for question generation? (y/n): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_t5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-0b691c02b9cb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(pdf_path, prompt, use_t5, output_lang)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Enter follow-up mode using the summary as context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- FOLLOW-UP MODE ---\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mhandle_followups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregated_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_t5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0b691c02b9cb>\u001b[0m in \u001b[0;36mhandle_followups\u001b[0;34m(valid_questions, summary_text, aggregated_context, use_t5, output_lang)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mfollow_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your follow-up question (or type 'done' to pick another question, or 'new' for a new random question):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfollow_up\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}